{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Ik7Sdn8dX5SEYXtz5w3EkWcuP1StOuQh",
      "authorship_tag": "ABX9TyNJVFzJvqneMyBsjcF1mXH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reecevinto/Cloud-based-PE-Malware-Detection-API/blob/main/S24_AISEC_Midterm_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJiPFsSZv9Vs",
        "outputId": "70268ee5-412f-40da-9619-8d25d78f4e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-14 11:13:21--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.113.9, 52.216.52.49, 3.5.28.175, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.113.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7619200000 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘X_train.dat.1’\n",
            "\n",
            "X_train.dat.1       100%[===================>]   7.10G  48.5MB/s    in 2m 40s  \n",
            "\n",
            "2024-03-14 11:16:01 (45.4 MB/s) - ‘X_train.dat.1’ saved [7619200000/7619200000]\n",
            "\n",
            "--2024-03-14 11:16:01--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 54.231.131.33, 3.5.25.83, 3.5.29.62, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|54.231.131.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904800000 (1.8G) [binary/octet-stream]\n",
            "Saving to: ‘X_test.dat.1’\n",
            "\n",
            "X_test.dat.1        100%[===================>]   1.77G  51.5MB/s    in 39s     \n",
            "\n",
            "2024-03-14 11:16:40 (46.9 MB/s) - ‘X_test.dat.1’ saved [1904800000/1904800000]\n",
            "\n",
            "--2024-03-14 11:16:41--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.216.29.12, 54.231.204.9, 54.231.133.129, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.216.29.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200000 (3.1M) [binary/octet-stream]\n",
            "Saving to: ‘y_train.dat.1’\n",
            "\n",
            "y_train.dat.1       100%[===================>]   3.05M  8.71MB/s    in 0.4s    \n",
            "\n",
            "2024-03-14 11:16:41 (8.71 MB/s) - ‘y_train.dat.1’ saved [3200000/3200000]\n",
            "\n",
            "--2024-03-14 11:16:41--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.225.17, 52.217.225.105, 52.217.136.81, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.225.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800000 (781K) [binary/octet-stream]\n",
            "Saving to: ‘y_test.dat.1’\n",
            "\n",
            "y_test.dat.1        100%[===================>] 781.25K  3.08MB/s    in 0.2s    \n",
            "\n",
            "2024-03-14 11:16:42 (3.08 MB/s) - ‘y_test.dat.1’ saved [800000/800000]\n",
            "\n",
            "--2024-03-14 11:16:42--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.225.17, 52.217.225.105, 52.217.136.81, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.225.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92160330 (88M) [text/csv]\n",
            "Saving to: ‘metadata.csv.1’\n",
            "\n",
            "metadata.csv.1      100%[===================>]  87.89M  39.2MB/s    in 2.2s    \n",
            "\n",
            "2024-03-14 11:16:44 (39.2 MB/s) - ‘metadata.csv.1’ saved [92160330/92160330]\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Collecting git+https://github.com/PFGimenez/ember.git\n",
            "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-3b8yqwx5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-3b8yqwx5\n",
            "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lief in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n",
            "MalConv(\n",
            "  (embed): Embedding(256, 8)\n",
            "  (conv1d): Conv1d(8, 128, kernel_size=(128,), stride=(128,))\n",
            "  (max_pool): MaxPool1d(kernel_size=128, stride=128, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Revised on 3/5/2024: Changed source files\n",
        "\n",
        "# This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development\n",
        "\n",
        "# Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
        "\n",
        "# Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "# Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
        "\n",
        "# Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "# Step 4:** Download the pre-processed training and test datasets.\n",
        "# ~8GB\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Step 5:** Copy the downloaded files to vMalConv\n",
        "!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv\n",
        "# Step 6:** Download and install Ember:\n",
        "!pip install git+https://github.com/PFGimenez/ember.git\n",
        "!pip install lief\n",
        "# Step 7:** Read vectorized features from the data files.\n",
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/vMalConv/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/vMalConv/\")\n",
        "# Step 8:** Get rid of rows with no labels.\n",
        "labelrows = (y_train != -1)\n",
        "X_train = X_train[labelrows]\n",
        "y_train = y_train[labelrows]\n",
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()\n",
        "!cp /content/X_train.h5 /content/drive/MyDrive/vMalConv/X_train.h5\n",
        "!cp /content/y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5\n",
        "# Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced.\n",
        "### Your code (optional) for sampling the original dataset.\n",
        "# Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MalConv(nn.Module):\n",
        "   def __init__(self, input_length=2000000, embedding_dim=8, window_size=128, output_dim=1):\n",
        "        super(MalConv, self).__init__()\n",
        "        self.embed = nn.Embedding(256, embedding_dim)  # 256 unique bytes, embedding dimension\n",
        "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=window_size, kernel_size=window_size, stride=window_size)\n",
        "        self.max_pool = nn.MaxPool1d(kernel_size=window_size)\n",
        "        self.fc1 = nn.Linear(in_features=window_size, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "   def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = x.transpose(1, 2)  # Conv1d expects (batch_size, channels, length)\n",
        "        x = self.conv1d(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Example of creating a MalConv model\n",
        "input_length = 2000000  # The fixed length for each input file\n",
        "model = MalConv(input_length=input_length)\n",
        "print(model)\n",
        "\n",
        "# Example input (a batch of byte sequences, padded or truncated to the fixed length)\n",
        "# For actual use, replace 'torch.randint' with your preprocessed byte sequence tensor\n",
        "#example_input = torch.randint(0, 256, (4, input_length), dtype=torch.long)  # 4 examples, random data\n",
        "#output = model(example_input)\n",
        "#print(output)  # The output probabilities for each example\n",
        "\n",
        "# Step 8:** Partial fit the standardScaler to avoid overloading the memory:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "mms = StandardScaler()\n",
        "for x in range(0,600000,100000):\n",
        "  mms.partial_fit(X_train[x:x+100000])\n",
        "X_train = mms.transform(X_train)\n",
        "## Reshape to create 3 channels ##\n",
        "import numpy as np\n",
        "X_train = np.reshape(X_train,(-1,1,2381))\n",
        "y_train = np.reshape(y_train,(-1,1,1))\n",
        "# Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets.\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming MalConv class definition is already provided as above\n",
        "\n",
        "# Convert your numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDatasets and DataLoaders for training and validation sets\n",
        "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
        "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
        "\n",
        "batch_size = 16  # Adjust based on your GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU.\n",
        "# Initialize the MalConv model\n",
        "model = MalConv()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "save_dir = \"drive/MyDrive/vMalConv/\"\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 10  # Adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Model checkpoint saved to {checkpoint_path}')\n",
        "\n",
        "# Task 3:** Complete the following code to evaluate your trained model on the test data.\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Convert test data to PyTorch tensors\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create a TensorDataset and DataLoader for test data\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store model predictions and actual labels\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels_batch in test_loader:\n",
        "        inputs, labels_batch = inputs.to(device), labels_batch.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(outputs)  # Round probabilities to obtain binary predictions\n",
        "        # Store predictions and labels\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "precision = precision_score(labels, predictions)\n",
        "recall = recall_score(labels, predictions)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "\n",
        "# After training the model, save it to a file\n",
        "torch.save(model.state_dict(), 'drive/MyDrive/vMalConv/trained_model.pth')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/trained_model.pth')\n",
        "\n",
        "# Define a function to classify PE files\n",
        "def classify_PE_file(file_path, model):\n",
        "    # Load the PE file and preprocess it (assuming you have a function for this)\n",
        "    # Example: processed_data = preprocess_PE_file(file_path)\n",
        "    processed_data = checkpoint_path(file_path)\n",
        "\n",
        "    # Convert processed data to Torch tensor\n",
        "    input_tensor = torch.tensor(processed_data, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Move input tensor to GPU if available\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        prediction = torch.round(output).item()  # Round probability to obtain binary prediction\n",
        "\n",
        "        # Map prediction to class label\n",
        "        if prediction == 1:\n",
        "            return \"Malware\"\n",
        "        else:\n",
        "            return \"Benign\"\n",
        "\n",
        "# Example usage of the function\n",
        "file_path = \"/content/drive/MyDrive/vMalConv/.ipynb_checkpoints.exe\"\n",
        "result = classify_PE_file(file_path, model)\n",
        "print(\"Classification result:\", result)\n",
        "\n",
        "# Import the module\n",
        "import malware_detection_app\n",
        "\n",
        "# Run the Streamlit app\n",
        "malware_detection_app.main()\n",
        "\n",
        "# Task 4:** Comment on the results in this text box.\n",
        "# Task 4: Comment on the results\n",
        "# Evaluate model performance\n",
        "print(\"Evaluation on Test Data:\")\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "\n",
        "# Comment on the results\n",
        "if accuracy >= 0.9:\n",
        "    print(\"The model achieved high accuracy on the test data, indicating robust performance.\")\n",
        "elif accuracy >= 0.8:\n",
        "    print(\"The model achieved moderate accuracy on the test data, suggesting decent performance.\")\n",
        "else:\n",
        "    print(\"The model did not perform well on the test data, further optimization may be required.\")\n",
        "\n",
        "if precision >= 0.9:\n",
        "    print(\"The model achieved high precision, indicating a low false positive rate.\")\n",
        "elif precision >= 0.8:\n",
        "    print(\"The model achieved moderate precision, suggesting a reasonable trade-off between false positives and true positives.\")\n",
        "else:\n",
        "    print(\"The model's precision is low, indicating a high false positive rate.\")\n",
        "\n",
        "if recall >= 0.9:\n",
        "    print(\"The model achieved high recall, indicating a low false negative rate.\")\n",
        "elif recall >= 0.8:\n",
        "    print(\"The model achieved moderate recall, suggesting a reasonable trade-off between false negatives and true positives.\")\n",
        "else:\n",
        "    print(\"The model's recall is low, indicating a high false negative rate.\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}